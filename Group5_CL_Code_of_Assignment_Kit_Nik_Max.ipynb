{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Group5_CL_Code_of_Assignment_Kit_Nik_Max.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-rc0Esraa95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6jAgtT6MYpQ",
        "colab_type": "text"
      },
      "source": [
        "If using Google Colab then this will be necessary, put the input files in the same way as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ldKQ2kBaql8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCxdmS-icfJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import os\n",
        "#os.chdir(\"drive/My Drive/CL_Dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKWWbXINck_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_81C0hMhI-o",
        "colab_type": "text"
      },
      "source": [
        "Because of the computation time we used a smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV4rN6KLaa-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_path = './train1_corpus.json'\n",
        "#test_path = './test1_corpus.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ6UVdnAhXlf",
        "colab_type": "text"
      },
      "source": [
        "### Test / Train Set Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qraiwzahX5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = './train_corpus.json'\n",
        "test_path = './test_corpus.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-7BcV4og5df",
        "colab_type": "text"
      },
      "source": [
        "### Actual Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjmq_YZ6aa-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Corpus(object):\n",
        "    \n",
        "    \"\"\"\n",
        "    This class creates a corpus object read off a .json file consisting of a list of lists,\n",
        "    where each inner list is a sentence encoded as a list of strings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, t, n=3, bos_eos=True, vocab=None):\n",
        "        \n",
        "        \"\"\"\n",
        "        DON'T TOUCH THIS CLASS! \n",
        "        IT'S HERE TO SHOW THE PROCESS, YOU DON'T NEED TO ANYTHING HERE. \n",
        "        \n",
        "        A Corpus object has the following attributes:\n",
        "         - vocab: set or None (default). If a set is passed, words in the input file not \n",
        "                         found in the set are replaced with the UNK string\n",
        "         - path: str, the path to the .json file used to build the corpus object\n",
        "         - t: int, words with frequency count < t are replaced with the UNK string\n",
        "         - ngram_size: int, 2 for bigrams, 3 for trigrams, and so on.\n",
        "         - bos_eos: bool, default to True. If False, bos and eos symbols are not \n",
        "                     prepended and appended to sentences.\n",
        "         - sentences: list of lists, containing the input sentences after lowercasing and \n",
        "                         splitting at the white space\n",
        "         - frequencies: Counter, mapping tokens to their frequency count in the corpus\n",
        "        \"\"\"\n",
        "        \n",
        "        self.vocab = vocab        \n",
        "        self.path = path\n",
        "        self.t = t\n",
        "        self.ngram_size = n\n",
        "        self.bos_eos = bos_eos\n",
        "        \n",
        "        self.sentences = self.read()\n",
        "        # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
        "    \n",
        "        self.frequencies = self.freq_distr()\n",
        "        # output --> Counter('the': 485099, 'of': 301877, 'i': 286549, ...)\n",
        "        # the numbers are made up, they aren't the actual frequency counts\n",
        "        \n",
        "        if self.t or self.vocab:\n",
        "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
        "            self.sentences = self.filter_words()\n",
        "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'UNK', '.'], ...]\n",
        "            # supposing that park wasn't frequent enough or was outside of the training \n",
        "            # vocabulary, it gets replaced by the UNK string\n",
        "            \n",
        "        if self.bos_eos:\n",
        "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
        "            self.sentences = self.add_bos_eos()\n",
        "            # output --> [['bos', i', 'am', 'home' '.', 'eos'], \n",
        "            #             ['bos', you', 'went', 'to', 'the', 'park', '.', 'eos'], ...]\n",
        "                    \n",
        "    def read(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        Reads the sentences off the .json file, replaces quotes, lowercases strings and splits \n",
        "        at the white space. Returns a list of lists.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.path.endswith('.json'):\n",
        "            sentences = json.load(open(self.path, 'r', encoding ='ISO-8859-1'))                \n",
        "        else:   \n",
        "            sentences = []\n",
        "            with open(self.path, 'r', encoding ='ISO-8859-1') as f:\n",
        "                for line in f:\n",
        "                    print(line[:20])\n",
        "                    # first strip away newline symbols and the like, then replace ' and \" with the empty \n",
        "                    # string and get rid of possible remaining trailing spaces \n",
        "                    line = line.strip().translate({ord(i): None for i in '\"\\'\\\\'}).strip(' ')\n",
        "                    # lowercase and split at the white space (the corpus has ben previously tokenized)\n",
        "                    sentences.append(line.lower().split(' '))\n",
        "        \n",
        "        return sentences\n",
        "    \n",
        "    def freq_distr(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        Creates a counter mapping tokens to frequency counts\n",
        "        \n",
        "        count = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            for word in sentence:\n",
        "                count[w] += 1\n",
        "            \n",
        "        \"\"\"\n",
        "    \n",
        "        return Counter([word for sentence in self.sentences for word in sentence])\n",
        "        \n",
        "    \n",
        "    def filter_words(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        Replaces illegal tokens with the UNK string. A token is illegal if its frequency count\n",
        "        is lower than the given threshold and/or if it falls outside the specified vocabulary.\n",
        "        The two filters can be both active at the same time but don't have to be. To exclude the \n",
        "        frequency filter, set t=0 in the class call.\n",
        "        \"\"\"\n",
        "        \n",
        "        filtered_sentences = []\n",
        "        for sentence in self.sentences:\n",
        "            filtered_sentence = []\n",
        "            for word in sentence:\n",
        "                if self.t and self.vocab:\n",
        "                    # check that the word is frequent enough and occurs in the vocabulary\n",
        "                    filtered_sentence.append(\n",
        "                        word if self.frequencies[word] > self.t and word in self.vocab else 'UNK'\n",
        "                    )\n",
        "                else:\n",
        "                    if self.t:\n",
        "                        # check that the word is frequent enough\n",
        "                        filtered_sentence.append(word if self.frequencies[word] > self.t else 'UNK')\n",
        "                    else:\n",
        "                        # check if the word occurs in the vocabulary\n",
        "                        filtered_sentence.append(word if word in self.vocab else 'UNK')\n",
        "                        \n",
        "            if len(filtered_sentence) > 1:\n",
        "                # make sure that the sentence contains more than 1 token\n",
        "                filtered_sentences.append(filtered_sentence)\n",
        "    \n",
        "        return filtered_sentences\n",
        "    \n",
        "    def add_bos_eos(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        Adds the necessary number of BOS symbols and one EOS symbol.\n",
        "        \n",
        "        In a bigram model, you need one bos and one eos; in a trigram model you need two bos and one eos, \n",
        "        and so on...\n",
        "        \"\"\"\n",
        "        \n",
        "        padded_sentences = []\n",
        "        for sentence in self.sentences:\n",
        "            padded_sentence = ['#bos#']*(self.ngram_size-1) + sentence + ['#eos#']\n",
        "            padded_sentences.append(padded_sentence)\n",
        "    \n",
        "        return padded_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGlXlw0haa-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM(object):\n",
        "    \n",
        "    \"\"\"\n",
        "    Creates a language model object which can be trained and tested.\n",
        "    The language model has the following attributes:\n",
        "     - vocab: set of strings\n",
        "     - lam: float, indicating the constant to add to transition counts to smooth them (default to 1)\n",
        "     - ngram_size: int, the size of the ngrams\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n=3, vocab=None):\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.ngram_size = n\n",
        "      \n",
        "    def get_ngram(self, sentence, i):\n",
        "        \n",
        "        \"\"\"\n",
        "        CHANGE AT OWN RISK.\n",
        "        \n",
        "        Takes in a list of string and an index, and returns the history and current \n",
        "        token of the appropriate size: the current token is the one at the provided \n",
        "        index, while the history consists of the n-1 previous tokens. If the ngram \n",
        "        size is 1, only the current token is returned.\n",
        "        \n",
        "        Example:\n",
        "        input sentence: ['bos', 'i', 'am', 'home', 'eos']\n",
        "        target index: 2\n",
        "        ngram size: 3\n",
        "        \n",
        "        ngram = ['bos', 'i', 'am']  \n",
        "        #from index 2-(3-1) = 0 to index i (the +1 is just because of how Python slices lists) \n",
        "        \n",
        "        history = ('bos', 'i')\n",
        "        target = 'am'\n",
        "        return (('bos', 'i'), 'am')\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.ngram_size == 1:\n",
        "            return sentence[i]\n",
        "        else:\n",
        "            ngram = sentence[i-(self.ngram_size-1):i+1]\n",
        "            history = tuple(ngram[:-1])\n",
        "            target = ngram[-1]\n",
        "            return (history, target)\n",
        "                    \n",
        "    def update_counts(self, corpus):\n",
        "        \n",
        "        \"\"\"\n",
        "        CHANGE AT OWN RISK.\n",
        "        \n",
        "        Creates a transition matrix with counts in the form of a default dict mapping history\n",
        "        states to current states to the co-occurrence count (unless the ngram size is 1, in which\n",
        "        case the transition matrix is a simple counter mapping tokens to frequencies. \n",
        "        The ngram size of the corpus object has to be the same as the language model ngram size.\n",
        "        The input corpus (passed by providing the corpus object) is processed by extracting ngrams\n",
        "        of the chosen size and updating transition counts.\n",
        "        \n",
        "        This method creates three attributes for the language model object:\n",
        "         - counts: dict, described above\n",
        "         - vocab: set, containing all the tokens in the corpus\n",
        "         - vocab_size: int, indicating the number of tokens in the vocabulary\n",
        "        \"\"\"\n",
        "        \n",
        "        # Removed this ValueError as we are using interpolation with tri, bi and unigrams.\n",
        "        \"\"\"if self.ngram_size != corpus.ngram_size:\n",
        "            raise ValueError(\"The corpus was pre-processed considering an ngram size of {} while the \"\n",
        "                             \"language model was created with an ngram size of {}. \\n\"\n",
        "                             \"Please choose the same ngram size for pre-processing the corpus and fitting \"\n",
        "                             \"the model.\".format(corpus.ngram_size, self.ngram_size))\"\"\"\n",
        "\n",
        "        #This was used to check if the ngram_size was correct\n",
        "        #print(\"TRI: \", self.ngram_size) \n",
        "        \n",
        "        \"\"\"A dictionary is made for the trigrams, \n",
        "        every trigrams gets counted and if it is not in the dictionary yet it will be added\"\"\"\n",
        "        \n",
        "        self.counts_tri = defaultdict(dict) if self.ngram_size > 1 else Counter()\n",
        "        for sentence in corpus.sentences:\n",
        "            for idx in range(self.ngram_size-1, len(sentence)):\n",
        "                ngram = self.get_ngram(sentence, idx)\n",
        "                if self.ngram_size == 1:\n",
        "                    self.counts_tri[ngram] += 1\n",
        "                else:\n",
        "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
        "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
        "                    # the first time, after that everything runs smoothly\n",
        "                    try:\n",
        "                        self.counts_tri[ngram[0]][ngram[1]] += 1\n",
        "                    except KeyError:\n",
        "                        self.counts_tri[ngram[0]][ngram[1]] = 1\n",
        "\n",
        "        #We now move on to make a dictionary for the bigrams, so we decrease the ngram_size by one.\n",
        "        self.ngram_size -= 1\n",
        "        #This was used to check if the ngram_size changed correctly\n",
        "        #print(\"BI: \", self.ngram_size) \n",
        "      \n",
        "        \"\"\"A dictionary is made for the bigrams, \n",
        "        every bigrams gets counted and if it is not in the dictionary yet it will be added\"\"\"\n",
        "\n",
        "        self.counts_bi = defaultdict(dict) if self.ngram_size > 1 else Counter()\n",
        "        for sentence in corpus.sentences:\n",
        "            for idx in range(self.ngram_size-1, len(sentence)):\n",
        "                ngram = self.get_ngram(sentence, idx)\n",
        "                if self.ngram_size == 1:\n",
        "                    self.counts_bi[ngram] += 1\n",
        "                else:\n",
        "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
        "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
        "                    # the first time, after that everything runs smoothly\n",
        "                    try:\n",
        "                        self.counts_bi[ngram[0]][ngram[1]] += 1\n",
        "                    except KeyError:\n",
        "                        self.counts_bi[ngram[0]][ngram[1]] = 1\n",
        "\n",
        "        #We now move on to make a counter for unigrams, so we decrease the ngram_size by one. \n",
        "        self.ngram_size -= 1 \n",
        "        #This was used to check if the ngram_size changed correctly \n",
        "        #print(\"UNI: \", self.ngram_size) \n",
        "\n",
        "        \"\"\"A counter is made for the unigrams, \n",
        "        every unigram gets counted and if it is not in the counter yet it will be added\"\"\"\n",
        "\n",
        "        self.counts = defaultdict(dict) if self.ngram_size > 1 else Counter()\n",
        "        for sentence in corpus.sentences:\n",
        "            for idx in range(self.ngram_size-1, len(sentence)):\n",
        "                ngram = self.get_ngram(sentence, idx)\n",
        "                if self.ngram_size == 1:\n",
        "                    self.counts[ngram] += 1\n",
        "                else:\n",
        "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
        "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
        "                    # the first time, after that everything runs smoothly\n",
        "                    try:\n",
        "                        self.counts[ngram[0]][ngram[1]] += 1\n",
        "                    except KeyError:\n",
        "                        self.counts[ngram[0]][ngram[1]] = 1\n",
        "\n",
        "        #We revert the ngram_size back to the original as that is needed to run the Perplexity function properly\n",
        "        self.ngram_size += 2\n",
        "        #This was used to check if the ngram_size changed correctly  \n",
        "        #print(\"Final: \",self.ngram_size )   \n",
        "        \n",
        "        # first loop through the sentences in the corpus, than loop through each word in a sentence\n",
        "        self.vocab = {word for sentence in corpus.sentences for word in sentence}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "    \n",
        "    def get_unigram_probability(self, ngram):\n",
        "        \n",
        "        \"\"\"\n",
        "        CHANGE THIS.\n",
        "        Compute the probability of a given unigram in the estimated language model using\n",
        "        Laplace smoothing (add k).\n",
        "        \"\"\"\n",
        "        \n",
        "        tot = sum(list(self.counts.values()))\n",
        "        try:\n",
        "            ngram_count = self.counts[ngram]\n",
        "        except KeyError:\n",
        "            ngram_count = 0\n",
        "            print(ngram_count, tot)\n",
        "        \n",
        "        return ngram_count/tot\n",
        "    \n",
        "    def get_ngram_probability(self, history, target):\n",
        "\n",
        "        \"\"\"First we compute the probabilities of the trigram.\n",
        "        The amount of times the specific trigram with [history][target] will be divided by the total amount of trigrams with [history]. \n",
        "        If there is no count then the number will be set to 0.\n",
        "        Probabilties are put in the tri variable\"\"\"\n",
        "\n",
        "        ngram_tot_tri = np.sum(list(self.counts_tri[history].values()))\n",
        "        try:\n",
        "            transition_count_tri = self.counts_tri[history][target]\n",
        "        except KeyError:\n",
        "            transition_count_tri = 0\n",
        "\n",
        "        \"\"\"We had some issues with the trigram, as we would sometimes get NaN (not a number) values, \n",
        "        so then the model would crash because it only accepts numbers. \n",
        "        If there was a NaN value, which can be observed when we try to see if the value is not equal to itself, \n",
        "        a NaN would return true, then we replaced it with 0.\n",
        "        \"\"\"\n",
        "        tri = transition_count_tri/ngram_tot_tri\n",
        "        if tri != tri:\n",
        "          tri = 0\n",
        "\n",
        "        \"\"\"As the current history is meant for trigrams, we have to make it suitable for bigrams.\n",
        "        This can be done by just taking the second word for the history and target should be the same.\n",
        "        \n",
        "        Example: \n",
        "        history = (\"scary\", \"dog\") \n",
        "        target = \"barks\"\n",
        "        history_bi = (\"dog\")\n",
        "        target = \"barks\"\n",
        "\n",
        "        history = (\"dog\", \"barks\")\n",
        "        target = \"menacingly\"\n",
        "        history_bi = (\"barks\",)\n",
        "        target = \"menacingly\"    \"\"\"\n",
        "\n",
        "        history_bi = history[1:2]\n",
        "\n",
        "        \"\"\"Now we compute the probabilities of the bigram.\n",
        "        The amount of times the specific bigram with [history][target] will be divided by the total amount of bigrams with [history]. \n",
        "        If there is no count then the number will be set to 0.\n",
        "        Probabilties are put in the bi variable\"\"\"\n",
        "\n",
        "        ngram_tot_bi = np.sum(list(self.counts_bi[history_bi].values()))\n",
        "        try:\n",
        "            transition_count_bi = self.counts_bi[history_bi][target]\n",
        "        except KeyError:\n",
        "            transition_count_bi = 0\n",
        "\n",
        "        bi = transition_count_bi/ngram_tot_bi\n",
        "\n",
        "        \"\"\"Now we compute the probabilities of the unigram.\n",
        "        It is the count of a specific unigram divided by the total amount of unigrams.\n",
        "        Probabilities are saved in uni\"\"\"\n",
        "\n",
        "        tot = sum(list(self.counts.values()))\n",
        "        ngram_count = self.counts[target]\n",
        "\n",
        "        uni = ngram_count/tot\n",
        "\n",
        "        \"\"\"These are weights for the interpolation, \n",
        "        l1 is for the unigram, l2 is for the bigram and l3 is for the trigram\"\"\"\n",
        "        l1 = 0.3\n",
        "        l2 = 0.4\n",
        "        l3 = 0.3\n",
        "\n",
        "        return (uni*l1) + (bi*l2) + (tri*l3) \n",
        "    \n",
        "    def perplexity(self, test_corpus):\n",
        "        \n",
        "        \"\"\"\n",
        "        Uses the estimated language model to process a corpus and computes the perplexity \n",
        "        of the language model over the corpus.\n",
        "        \n",
        "        DON'T TOUCH THIS FUNCTION!!!\n",
        "        \"\"\"\n",
        "        probs = []\n",
        "        for sentence in test_corpus.sentences:\n",
        "            for idx in range(self.ngram_size-1, len(sentence)):\n",
        "                ngram = self.get_ngram(sentence, idx)\n",
        "                if self.ngram_size == 1:\n",
        "                    probs.append(self.get_unigram_probability(ngram))\n",
        "                else:\n",
        "                    probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
        "        \n",
        "        entropy = np.log2(probs)\n",
        "        # this assertion makes sure that you retrieved valid probabilities, whose log must be <= 0\n",
        "        assert all(entropy <= 0)\n",
        "        \n",
        "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
        "        \n",
        "        return pow(2.0, avg_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTUJN7yzhqat",
        "colab_type": "text"
      },
      "source": [
        "### Implementing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEDSDVAbPWzz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grQ6ROewaa-Q",
        "colab_type": "code",
        "outputId": "157ea965-01dd-4569-fbe1-9074cfcc9a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# We set n = 3 everywhere so in the corpus and the model classes, as we were not allowed to have any free parameters.\n",
        "\n",
        "train_corpus = Corpus(train_path, 10, n=3 , bos_eos=True, vocab=None)\n",
        "bigram_model = LM(n=3)\n",
        "bigram_model.update_counts(train_corpus)\n",
        "\n",
        "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
        "test_corpus = Corpus(test_path, None, n=3, bos_eos=True, vocab=bigram_model.vocab)\n",
        "bigram_model.perplexity(test_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:183: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180.35761362138015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}